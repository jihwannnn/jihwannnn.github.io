---
title: OUTTER
author: Jihwan
date: 2024-07-19
category: Jekyll
layout: post
---

### 9.1 RNN
* 순환신경망. 이름 그대로 내부에서 순환되는 구조를 갖는다.

<br>**Vanilla RNN (가장 기본적인 RNN 구조)**
![RNN image 1](/assets/image/9_RNN/RNN_1.jpg)

* $x_t$: 입력 값
* $y_t$: 출력 값
* $h_t$: Hidden State (은닉 상태)

* 은닉층의 계산 값이 출력층과 다음 은닉층에 전달됨
* 각 층이 하나의 step

<br>![RNN image 2](/assets/image/9_RNN/RNN_2.png)

$h_t = f_h(W_{xh}x_t + W_{hh}h_{t-1} + b_t)$

$y_t = f_y(W_{hy}h_t + b_y)$

![RNN image 3](/assets/image/9_RNN/RNN_3.png)

* 요소의 shape
    * $x_t$: ($d \times 1$)
    * $W_x$: ($D_h \times d$)
    * $W_h$: ($D_h \times D_h$)
    * $W_y$: ($D_y \times D_h$)
    * $h_{t-1}$: ($D_h \times 1$)
    * $b$: ($D_h \times 1$)

<br>**RNN - 한계**
![RNN image 4](/assets/image/9_RNN/RNN_4.png)

* 결국 Gradient Vanishing, Gardient Exploding 문제를 피할 수 없다..

<br>**RNN - 결과**
![RNN image 5](/assets/image/9_RNN/RNN_5.png)

### 9.2 LSTM & GRU
* **LSTM**: Cell state를 도입해 기울기 소실 문제를 완화

![LSTM image 1](/assets/image/9_RNN/LSTM_1.png)

* **GRU**: Cell state와 hidden state를 통합
![GRU image 1](/assets/image/9_RNN/GRU_1.png)

### 9.3 Seq2Seq
* RNN의 한계
    * Gardient Vanishing: 과거 정보 소실
    * Casuality(인과율): 미래의 정보를 참고할 수 없다

<br>**Seq2Seq**

![Seq2Seq image 1](/assets/image/9_RNN/Seq2Seq_1.png)

* RNN을 조립해서 만듬
* 그림의 앞 부분을 인코더, 뒷 부분을 디코더라 할 때 인코더의 마지막 시점 hidden state를 context vector라 하고, 그것을 디코더의 첫 hidden state로 넣어준다.

* **Greedy Decoding**: input에 이전 시점의 output을 넣는다. output은 가장 높은 확률 값을 갖는 토큰으로 선정한다.
* **Teacher Forcing**: 디코더의 입력을 정답 시퀀스로 한다. output으로 오답이 나왔을 때의 영향을 방지할 수 있다.

---
### 10.1 Attention in Seq2Seq