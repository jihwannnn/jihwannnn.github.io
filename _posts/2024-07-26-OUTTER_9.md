---
title: OUTTER
author: Jihwan
date: 2024-07-19
category: Jekyll
layout: post
---

### 9.1 RNN
* 순환신경망. 이름 그대로 내부에서 순환되는 구조를 갖는다.

<br>**Vanilla RNN (가장 기본적인 RNN 구조)**

![RNN image 1](/assets/image/9_RNN/RNN_1.jpg)

* $x_t$: 입력 값
* $y_t$: 출력 값
* $h_t$: Hidden State (은닉 상태)

* 은닉층의 계산 값이 출력층과 다음 은닉층에 전달됨
* 각 층이 하나의 step

<br>![RNN image 2](/assets/image/9_RNN/RNN_2.png)

$h_t = f_h(W_{xh}x_t + W_{hh}h_{t-1} + b_t)$

$y_t = f_y(W_{hy}h_t + b_y)$

![RNN image 3](/assets/image/9_RNN/RNN_3.png)

* 요소의 shape
    * $x_t$: ($d \times 1$)
    * $W_x$: ($D_h \times d$)
    * $W_h$: ($D_h \times D_h$)
    * $W_y$: ($D_y \times D_h$)
    * $h_{t-1}$: ($D_h \times 1$)
    * $b$: ($D_h \times 1$)

<br>**RNN - 한계**

![RNN image 4](/assets/image/9_RNN/RNN_4.png)

* 결국 Gradient Vanishing, Gardient Exploding 문제를 피할 수 없다..

<br>**RNN - 결과**

![RNN image 5](/assets/image/9_RNN/RNN_5.png)

### 9.2 LSTM & GRU
* **LSTM**: Cell state를 도입해 기울기 소실 문제를 완화

![LSTM image 1](/assets/image/9_RNN/LSTM_1.png)

* **GRU**: Cell state와 hidden state를 통합

![GRU image 1](/assets/image/9_RNN/GRU_1.png)

### 9.3 Seq2Seq
* RNN의 한계
    * Gardient Vanishing: 과거 정보 소실
    * Casuality(인과율): 미래의 정보를 참고할 수 없다

<br>**Seq2Seq**

![Seq2Seq image 1](/assets/image/9_RNN/Seq2Seq_1.png)

* RNN을 조립해서 만듬
* 그림의 앞 부분을 인코더, 뒷 부분을 디코더라 할 때 인코더의 마지막 시점 hidden state를 context vector라 하고, 그것을 디코더의 첫 hidden state로 넣어준다.

* **Greedy Decoding**: input에 이전 시점의 output을 넣는다. output은 가장 높은 확률 값을 갖는 토큰으로 선정한다.
* **Teacher Forcing**: 디코더의 입력을 정답 시퀀스로 한다. output으로 오답이 나왔을 때의 영향을 방지할 수 있다.

---
### 10.1 Attention

* Seq2Seq의 문제
    * context vector가 마지막 노드의 hidden state이기 때문에 앞쪽 input token의 정보가 희미해진다..
    * 이를 해결하기 위해 Attention 매커니즘을 적용

<br>**Attention**

![Attention image 1](/assets/image/10_Attention/Attention_1.png)

* 전체적인 과정

    [1] Attention Score를 구한다.
    <ul>
        <li>$attention\ score = \vec{e_t} = H^eW_\alpha \vec{h_t^d}$</li>
        <li>$H^e$: 인코더의 모든 hidden state를 하나의 행렬로 표현, (T, D). (T: 인코더 셀의 갯수, D: hidden state의 차원)</li>
        <li>$W_\alpha$: 가중치 행렬, (D, D)</li>
        <li>$\vec{h_t^d}$: 디코더 t step에서의 hidden state (D 차원)</li>
    </ul>

    <br>[2] Attention Score를 Softmax 함수에 통과시켜서 Attention Weight($= \alpha$)를 구한다.

    [3] 각 인코더의 Attention Weight와 hidden state를 가중합하여 Context Vector (Attention Value)를 구한다.
    <ul>
        <li>$c_t = \sum_{j=1}^T{\alpha_j}{h_j^e}$</li>
    </ul>

    <br>[4] Context Vector와 Output($y_t$)을 연결(Concatenate)한다.

<style>
    li {
        margin: 5px;
    }
</style>

---
### 11.1 Transformer

* ~~진짜 이걸 보면서 뇌가 많이 아팠다.. 우울하고.. 기운이 없어지고.. 나중에 내 미래가 걱정되기 시작했다...~~

* 참고자료
    * 이 부분은 자료를 많이 참고하자
    * *[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)*
    * *[wikidocs](https://wikidocs.net/book/2155)*

<br>![Transformer image 1](/assets/image/11_Transformer/Transformer_1.png)

* 특징
    * 토큰을 순차적으로 받는게 아닌 하나의 문장을 한번에 입력 받아 처리한다. RNN 구조가 아닌 인코더-디코더 구조를 하고있다.
    * 입력을 동적으로 받을 수 있다. 패딩과 마스킹을 사용한다.

<br>

***크게는 인코더와 디코더로 나뉜다***
* 인코더: Multihead Attention과 Feed Forward로 이루어진 층이 N번 반복
* 디코더: Masked Multihead Attention과 Multihead Attention, Feed Forward로 이루어진 층이 N번 반복

<br>

* **구성 요소**
    * Multihead Attention
    * Add & Norm
    * Feed Forward
    * Masked Multihead Attention
    * Positional Encoding
    * Linear
    * Softmax


<br>

* **Positional Encoding**: 위치 정보를 추가해준다

<br>

* **Add & Norm**
    * Add: Residual Connection, Gradient Vanishing을 해결하기 위해 함수의 결과에 identity를 더해주는 방식. $H(\mathbf{x}) = F(\mathbf{x}) + \mathbf{x}$
    * Norm: Batch와 Layer 두 가지가 있지만 Layer Normalization을 사용. 데이터 내의 정규화를 적용한다.

<br>

* **Feed Forward**: Linear -> ReLU -> Linear로 이루어진 sublayer. 모델의 표현력을 높여준다

***Attention in transformer***

![Transformer image 2](/assets/image/11_Transformer/Transformer_2.png)

* 어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구한다. 그리고 구해낸 이 유사도를 가중치로 하여 키와 맵핑되어있는 각각의 '값(Value)'에 반영한다. 그리고 유사도가 반영된 '값(Value)'을 모두 가중합하여 리턴하게된다.

<br>

* 기존에 Seq2Seq에서 사용한 Q, K, V에 관한 내용이다
```
Q = Query : t 시점의 디코더 셀에서의 은닉 상태
K = Keys : 모든 시점의 인코더 셀의 은닉 상태들
V = Values : 모든 시점의 인코더 셀의 은닉 상태들
```
<br>

* Transformer에서는 이것을 확장하여 사용한다. hidden state라는 것도 결국 입력에 대한 행렬의 linear 연산에 불과하다. 따라서 Q, K, V도 입력에 대한 하나의 연산의 결과들로 볼 수 있다.
```
Q : 입력 문장의 모든 단어 벡터들
K : 입력 문장의 모든 단어 벡터들
V : 입력 문장의 모든 단어 벡터들
```
<br>

* Scaled Dot Product Attention: Attention Score를 $\sqrt{d}$로 나누어 softmax를 취한 값이 0에 가까운 값이 되는 것을 방지한다. <br>
    $C = Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d}})V$

<br>

* Multihead Attention: 하나의 Attetion Head를 여러 개(h)의 Head로 쪼개는 방식. 단어 간의 연관성을 여러 각도에서 볼 수 있고, 병렬적으로 계산하여 Scaled Dot Product Attention보다 빠른 계산을 할 수 있다. <br>
    $C = MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O$
    $\quad where \ head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$
    * 이때 가중치의 shape은 ($d,d^{\prime}$)이다 ($d^{\prime} = \lfloor \frac{d}{h} \rfloor$)
    * 따라서 $W^O$는 차원을 맞춰주기 위한 가중치이다.

<br>

* Maked Multihead Attention: 개연성을 위해 attention score를 계산할 때 미래의 정보를 masking 해준다.

---
### 12.1 BERT

![BERT image 1](/assets/image/12_BERT/BERT_1.png)

* **BERT**(Bidirectional Encoder Representation from Transfomers): 트랜스포머 기반 양방향 인코더 표현. Transformer 구조에서 인코더 부분만 가져와 사용한다 (Transformer의 인코더의 self attention 원리 이용). Sequence에 상관없이 모든 상관관계를 고려하여 class 확률을 출력한다.

<br>

* **GPT**(Generative Pre-Trained Transformer): 반대로 디코더 부분만 사용한다

<br>

* ELMo: 단순히 두 개의 Unidirectional 구조를 concat한 모델

![BERT image 2](/assets/image/12_BERT/BERT_2.png)

<br>

* NLG & NLP
    * **NLG**(Natural Language Generation): 문장을 생성하는 task, GPT가 적합하다.
    * **NLU**(Natural Language Understanding): 문장을 이해하는 task, BERT가 적합하다.

### 12.2 Pre-Training & Fine-Tuning

* 거대 데이터를 이용해 미리 모델을 훈련(**Pre-Training**)해두고, 사전 훈련된 모델을 필요한 Task에 맞게 튜닝(**Fine-Tuning**)하는 작업

<br>

* Feature Extraction: 새로 쌓은 부분의 파라미터만 업데이트 하는 것.

<br>

* Pre-Training 종류
    * MLM(Masked Language Model): 일정 토큰을 masking한 채 문장을 복원하도록 학습.
    * NSP(Next Sentence Prediction): 문장 간의 관계를 학습. [CLS]: 전체 시퀀스 분류 토큰, [SEP]: 시퀀스 내 문장 분류 토큰
    * Embedding Combination: 기존 Transformer의 embedding에 문장 encoding을 추가한 것.

---
### 13.1 GPT